
library ("ROCR")
library ("PresenceAbsence")
library("randomForest")
library("rms")
library("XLConnect")
library ("plyr")
library("RODBC")
library(plyr)
require(gbm)
require(caret)

####################################### Load data, understanding, DQ

setwd("/Users/dilip.patel/Google Drive/Learning/Analytics/Kaggle/Santander_Classification")

getwd()

# Importing data into R

train <- read.csv("train.csv")
predict <- read.csv("test.csv")
sample_submission <- read.csv("sample_submission.csv")

# price quotes from our suppliers
str(train)
#summary(train)
colnames(train)

str(predict)
#summary(test)
colnames(predict)

####################################### Preparing Training Data

colnameskeep_initial <- colnames(train[,(colnames(train) %in% colnames(predict))])
colnameskeep_initial <- c(colnameskeep_initial,"TARGET")
train = train[,colnameskeep_initial]

train$TARGET = factor(train$TARGET)

## set the seed to make your partition reproductible
set.seed(123)

trainIndex <- createDataPartition(train$TARGET, p = .8,
                                  list = FALSE,
                                  times = 1)



SantanderTrain <- train[ trainIndex,]
SantanderTest  <- train[-trainIndex,]

colnames(SantanderTrain)
#table(SantanderTrain$TARGET)

####################################### Training Model

RFmodel.rf <- randomForest(TARGET ~ .,data = SantanderTrain[-1],
                           importance=TRUE,
                           mtry=50, # # of variables (default sqrt(p))
                           #proximity=TRUE,
                           ntree=5000,
                           #nodesize=5, #Min # of observations (default 1) - do CV
                           #maxnodes=15, # Maximum number of terminal nodes trees (default max possible)
                           type="classification", 
                           #sampsize = c(250,250),
                           keep.forest=TRUE)

round(head(RFmodel.rf$err.rate,202),4)  	# look at the error rate
min.err <- min(data.frame(RFmodel.rf$err.rate)["OOB"])	# find tree with minnimum error rate
min.err								# Minimum error rate
min.err.index <- which(data.frame(RFmodel.rf$err.rate)["OOB"]==min.err)
min.err.index							# Tree with minimum error

RFmodel.rf
summary(RFmodel.rf)
# summary(RFmodel.rf$test$err.rate)
head(RFmodel.rf$err.rate)
plot(RFmodel.rf$err.rate[,1]) # total error, it's learning until a point that it goes up again, due to 1's
plot(RFmodel.rf$err.rate[,2]) # error for 0, it's learning, it is going down
plot(RFmodel.rf$err.rate[,3]) # error for 1, it doesn't learn

# to see the importance of the variables, not only in the general error, as well the contribution to the flag (0,1)
rn <- round(importance(RFmodel.rf), 2)
rn[order(rn[,3], decreasing=TRUE),]

# Plot variable importance
plot(RFmodel.rf, log="y")
varImpPlot(RFmodel.rf, n.var=20)
#MDSplot(RFmodel.rf, ytraining)
par(mfrow = c(2,2))

#RFmodel.rf$RFmodel.rf[n_tree]


#------------------------------> Train ROC

# Training ROC
probxRFtraining <- predict(RFmodel.rf, SantanderTrain[,-371], type="prob")
pred <- prediction(probxRFtraining[,"1"],matrix(SantanderTrain[,371])) 
perf <- performance(pred, measure = "tpr", x.measure = "fpr") 
#plot(perf, print.cutoffs.at=seq(0,1,by=0.01))
plot(perf, col=rainbow(10), colorize=TRUE, lwd=3)
title("ROC Training")
abline(v=0.2)
abline(h=0.6)
auctraining <- performance(pred, measure = "auc")@y.values[[1]]
legend("bottomright",legend=c(paste("RF (AUC=",formatC(auctraining,digits=4,format="f"),")",sep="")),  
       col=c("red"), lty=1)

# OOB ROC
pred <- prediction(RFmodel.rf$votes[,"1"],matrix(SantanderTrain[,371]))
perf <- performance(pred, measure = "tpr", x.measure = "fpr") 
aucOOB <- performance(pred, measure = "auc")@y.values[[1]]
#plot(perf, print.cutoffs.at=seq(0,1,by=0.01))
plot(perf, col=rainbow(10), colorize=TRUE, lwd=3)
title("ROC OOB")
abline(v=0.2)
abline(h=0.6)
legend("bottomright",legend=c(paste("RF (AUC=",formatC(aucOOB,digits=4,format="f"),")",sep="")),  
       col=c("red"), lty=1)
#plot(perf, lwd=3)

####################################### Preparing Test Data

colnameskeep_initial <- colnames(test[,(colnames(test) %in% colnames(train))])
test = test[,colnameskeep_initial]


####################################### Prediction

str(sample_submission)
colnames(predict)

colnames(SantanderTest)
probxgotest <- predict(RFmodel.rf, predict, type="prob",na.action=na.roughfix)

final <- data.frame(ID = predict[,"ID"], TARGET = probxgotest[,"1"])
getwd()
write.csv(final)

write.csv(final, "Santander_submission_v1.csv", row.names = F)


#----------------------------------> test ROC
probxRFtesttree <- predict(RFmodel.rf, SantanderTest[,-371], type="prob")
#probxRFtesttree <- predict(RFmodel.rf,rbind(xtraining[,],xtesttree[,]), type="prob")
pred <- prediction(probxRFtesttree[,"1"],matrix(SantanderTest[,371]))
perf <- performance(pred, measure = "tpr", x.measure = "fpr") 
#plot(perf, print.cutoffs.at=seq(0,1,by=0.05))
plot(perf, col=rainbow(10), colorize=TRUE, lwd=3)
#plot(perf, lwd=3)

title("ROC Test Tree")
abline(v=0.2)
abline(h=0.6)
auctest <- performance(pred, measure = "auc")@y.values[[1]]

legend("bottomright",legend=c(paste("RF (AUC=",formatC(auctest,digits=4,format="f"),")",sep="")),  
       col=c("red"), lty=1)
