
library ("ROCR")
library ("PresenceAbsence")
library("randomForest")
library("rms")
library("XLConnect")
library ("plyr")
library("RODBC")
library(plyr)
require(gbm)
require(caret)

####################################### Load data, understanding, DQ

setwd("/Users/dilip.patel/Google Drive/Learning/Analytics/Kaggle/Santander_Classification")

getwd()

# Importing data into R

train <- read.csv("train.csv")
predict <- read.csv("test.csv")
sample_submission <- read.csv("sample_submission.csv")

# price quotes from our suppliers
str(train)
summary(train)
colnames(train)

summary(train)

str(predict)
#summary(test)
colnames(predict)

####################################### Preparing Training Data

colnameskeep_initial <- colnames(train[,(colnames(train) %in% colnames(predict))])
colnameskeep_initial <- c(colnameskeep_initial,"TARGET")
train = train[,colnameskeep_initial]
train <- train[,-1]
colnames(train)

# Remove variables with no more than one value
for (i in 1:dim(train)[2]){
  if(length(unique(train[,i])) < 2){
    print(colnames(train)[i])
    train <- train[,-i]
    
  }
}

unique(train$ind_var9_ult1)
summary(train$ind_var46_0)

# Factor conversion
for (i in 1:dim(train)[2]){
  if(length(unique(train[,i])) < 3){
    print(colnames(train)[i])
    train[,i] <- factor(train[,i])
    summary(train[,i])
    
  }
}

unique(train$num_var29_0)
summary(train$num_meses_var13_medio_ult3)


# Remove variables with no more than one value
for (i in 1:dim(train)[2]){
  if(length(unique(train[,i])) == 3){
    print(colnames(train)[i])
    train[,i] <- factor(train[,i])
    summary(train[,i])
    
  }
}

unique(train$saldo_medio_var13_corto_ult3)

## set the seed to make your partition reproductible
set.seed(123)

trainIndex <- createDataPartition(train$TARGET, p = .8,
                                  list = FALSE,
                                  times = 1)



SantanderTrain <- train[ trainIndex,]
SantanderTest  <- train[-trainIndex,]

colnames(SantanderTrain)
#table(SantanderTrain$TARGET)

####################################### Training Model

RFmodel.rf <- randomForest(TARGET ~ .,data = SantanderTrain,
                           importance=TRUE,
                           #mtry=25, # # of variables (default sqrt(p))
                           #proximity=TRUE,
                           ntree=500,
                           #nodesize=5, #Min # of observations (default 1) - do CV
                           #maxnodes=15, # Maximum number of terminal nodes trees (default max possible)
                           type="classification", 
                           #sampsize = c(250,250),
                           keep.forest=TRUE)



round(head(RFmodel.rf$err.rate,202),4)  	# look at the error rate
min.err <- min(data.frame(RFmodel.rf$err.rate)["OOB"])	# find tree with minnimum error rate
min.err								# Minimum error rate
min.err.index <- which(data.frame(RFmodel.rf$err.rate)["OOB"]==min.err)
min.err.index							# Tree with minimum error

RFmodel.rf
summary(RFmodel.rf)
# summary(RFmodel.rf$test$err.rate)
head(RFmodel.rf$err.rate)
plot(RFmodel.rf$err.rate[,1]) # total error, it's learning until a point that it goes up again, due to 1's
plot(RFmodel.rf$err.rate[,2]) # error for 0, it's learning, it is going down
plot(RFmodel.rf$err.rate[,3]) # error for 1, it doesn't learn

# to see the importance of the variables, not only in the general error, as well the contribution to the flag (0,1)
rn <- round(importance(RFmodel.rf), 2)
rn[order(rn[,3], decreasing=TRUE),]

# Plot variable importance
plot(RFmodel.rf, log="y")
varImpPlot(RFmodel.rf, n.var=20)
#MDSplot(RFmodel.rf, ytraining)
par(mfrow = c(2,2))

#RFmodel.rf$RFmodel.rf[n_tree]


#------------------------------> Train ROC

# Training ROC
probxRFtraining <- predict(RFmodel.rf, SantanderTrain[,-370], type="prob")
pred <- prediction(probxRFtraining[,"1"],matrix(SantanderTrain[,370])) 
perf <- performance(pred, measure = "tpr", x.measure = "fpr") 
#plot(perf, print.cutoffs.at=seq(0,1,by=0.01))
plot(perf, col=rainbow(10), colorize=TRUE, lwd=3)
title("ROC Training")
abline(v=0.2)
abline(h=0.6)
auctraining <- performance(pred, measure = "auc")@y.values[[1]]
legend("bottomright",legend=c(paste("RF (AUC=",formatC(auctraining,digits=4,format="f"),")",sep="")),  
       col=c("red"), lty=1)

# OOB ROC
pred <- prediction(RFmodel.rf$votes[,"1"],matrix(SantanderTrain[,371]))
perf <- performance(pred, measure = "tpr", x.measure = "fpr") 
aucOOB <- performance(pred, measure = "auc")@y.values[[1]]
#plot(perf, print.cutoffs.at=seq(0,1,by=0.01))
plot(perf, col=rainbow(10), colorize=TRUE, lwd=3)
title("ROC OOB")
abline(v=0.2)
abline(h=0.6)
legend("bottomright",legend=c(paste("RF (AUC=",formatC(aucOOB,digits=4,format="f"),")",sep="")),  
       col=c("red"), lty=1)
#plot(perf, lwd=3)

#-----------------------------------> GBM

fitControl <- trainControl(
  method = "repeatedcv", # The resampling method
  number = 5, #number of folds
  repeats = 1)

TuneGrid <- expand.grid(n.trees = 500,interaction.depth = 11,shrinkage = 0.01,n.minobsinnode=10)

fit.gbm <- train(TARGET ~ ., data = SantanderTrain,
                 method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE,
                 weights = datat$weights,
                 tuneGrid = TuneGrid
                 #,metric="ROC"
)

summary(fit.gbm)

####################################### Preparing Test Data

colnameskeep_initial <- colnames(test[,(colnames(test) %in% colnames(train))])
test = test[,colnameskeep_initial]


####################################### Prediction

str(sample_submission)
colnames(predict)

colnames(SantanderTest)
probxgotest <- predict(RFmodel.rf, predict, type="prob",na.action=na.roughfix)

final <- data.frame(ID = predict[,"ID"], TARGET = probxgotest[,"1"])
getwd()

write.csv(final, "Santander_submission_v1.csv", row.names = F)


#----------------------------------> test ROC
probxRFtesttree <- predict(RFmodel.rf, SantanderTest[,-370], type="prob")
#probxRFtesttree <- predict(RFmodel.rf,rbind(xtraining[,],xtesttree[,]), type="prob")
pred <- prediction(probxRFtesttree[,"1"],matrix(SantanderTest[,370]))
perf <- performance(pred, measure = "tpr", x.measure = "fpr") 
#plot(perf, print.cutoffs.at=seq(0,1,by=0.05))
plot(perf, col=rainbow(10), colorize=TRUE, lwd=3)
#plot(perf, lwd=3)

title("ROC Test Tree")
abline(v=0.2)
abline(h=0.6)
auctest <- performance(pred, measure = "auc")@y.values[[1]]

legend("bottomright",legend=c(paste("RF (AUC=",formatC(auctest,digits=4,format="f"),")",sep="")),  
       col=c("red"), lty=1)
